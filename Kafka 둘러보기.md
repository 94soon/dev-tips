# Kafka 둘러보기

# 성능이 좋은 이유

- 참고
    - [아파치 카프카 문서 - 데이터 저장](https://kafka.apache.org/documentation/#persistence)
    - [아파치 카프카 문서 - 효율 극대화](https://kafka.apache.org/documentation/#maximizingefficiency)

## Linear Write/Read

일반적으로 디스크 접근은 메모리에 비해 매우 느리다고 알려져 있지만, 그건 랜덤 액세스일 때 얘기고 선형 접근(또는 순차 접근?)이라면 디스크의 속도도 상당히 쓸만하다. 때로는 메모리를 랜덤 액세스하는 것보다 요즘 나온 HDD에서의 순차 접근이 더 빠르다는 결과도 있다고 한다.

또한 순차 접근은 예측성이 좋아서 운영체제에 의해 엄청나게 최적화된다. 요즘 운영체제는 대량의 블록에서 데이터를 미리 읽는 read-ahead와 작은 쓰기 작업을 모아서 한 번에 쓰는 write-behind를 지원하므로 순차 접근만 가능하다면 디스크를 사용해도 성능적으로 나쁠 것이 없다.

**카프카가 데이터를 디스크에 쓰고 읽으면서도 처리량이 뛰어난 이유 중의 하나는, 카프카의 주요 용도인 메시지 큐의 특성상 데이터를 읽고 쓰는 방식을 순차 접근으로 구성할 수 있기 때문이다.**

## Page Cache

순차 접근이 아닌 경우에는 디스크 접근은 속도가 매우 느리므로 운영체제는 디스크 캐시를 위해 메인 메모리를 공격적으로 사용하게 되었다. 모든 디스크 읽기/쓰기는 이 디스크 캐시를 거치게 되고, 이 기능을 강제로 끄는 것은 쉽지 않으므로, 애플리케이션에서 내부적으로 캐시를 사용한다면 실제로는 같은 내용을 운영체제의 페이지 캐시(페이지 단위의 디스크 캐시)와 애플리케이션 내부 캐시에 중복 저장하게 된다.

게다가 JVM은 객체의 메모리 오버해드가 상당히 크고, 힙 내 데이터가 많아질 수록 GC도 느려지고 성가신 일이 된다.

따라서 **카프카는 힙 메모리 내에 객체로 저장하는 대신 바이트 구조로 압축하고 운영체제가 훌륭하게 최적화하고 있는 페이지 캐시를 활용해서 디스크에 저장하는 방식을 도입해서 성능을 높일 수 있었다.** 미사용 메모리에 대한 automatic access와 바이트 구조로의 압축을 통해 32G 장비에서 28-30G를 페이지 캐시로 활용할 수 있었고 GC 부담 없이 성능을 높였다. 게다가 이 **페이지 캐시는 애플리케이션 수준이 아니라 운영체제 수준에서 관리되므로, 애플리케이션 서비스 재시작 시에도 별도의 워밍업 없이도 좋은 성능을 낼 수 있다.**

**파일시스템에 쓴다고는 하지만 실제로는 커널의 페이지 캐시로 데이터를 전송한다고 볼 수 있다.**

## 단순한 자료 구조

메시지 큐에 데이터를 저장하는 자료 구조로 BTree를 사용하기도 하는데, BTree가 O(logN)으로 탐색 효율이 매우 뛰어나지만 데이터가 저장되는 곳이 메모리가 아니라 디스크라면 얘기가 달라진다.

**카프카는 BTree를 사용하지 않고 그냥 파일 끝에 데이터를 추가하는 단순한 자료 구조를 사용**한다. 이게 가능한 이유도 앞에서 살펴본 것처럼 디스크 사용 방식이 대부분 순차 접근이기 때문이다. 따라서 **그냥 파일 끝에 추가하므로 데이터 양이 늘어난다고 성능이 낮아질 일도 없으므로 사실 상 O(1)이라고 할 수 있고, 읽기 작업이 쓰기 작업을 블록킹 할 일도 없다.** 

따라서 **굳이 고성능의 디스크를 사용할 필요가 없으므로 같은 값으로 용량이 큰 디스크를 사용할 수 있고, 이를 통해 확보한 공간에 메시지를 장기간(기본은 일주일) 저장할 수 있으므로 다른 메시징 시스템과 차별화된 기능을 제공할 수 있게 된다.**

## 일괄 처리

메시지를 쓰거나 읽을 때 개별 건마다 처리하지 않고 여러 건을 묶어서 일괄(batch) 처리하는 단순한 최적화를 통해 막대한 성능 개선 효과를 볼 수 있다. **일괄 처리는 한 번 전송에 사용되는 네트워크 패킷의 크기를 확대하므로 네트워크 I/O 오버헤드르 줄이고, 대규모의 순차 디스크 작업으로 이어지며, 연속적인 메모리 블록으로 이어지는 등 효율 개선 효과가 매우 크다.**

## Zero Copy

일반적인 데이터 전송은 다음과 같은 흐름으로 수행된다.

1. 운영체제가 디스크의 데이터를 커널 공간의 페이지 캐시로 읽어들인다.
1. 애플리케이션은 커널 공간의 데이터를 사용자 공간의 버퍼로 읽어들인다.
1. 애플리케이션은 사용자 공간의 버퍼에 있는 데이터를 커널 공간에 쓰고 이어서 소켓 버퍼에 쓴다.
1. 운영체제는 소켓 버퍼에 있는 데이터를 복사해서 NIC(Network Interface Card) 버퍼에 쓴다.

이 방식에는 네 번의 복사와 두 번의 시스템 콜이 동원된다. 하지만 **[`sendfile`](http://man7.org/linux/man-pages/man2/sendfile.2.html)을 사용하면 운영체제가 페이지 캐시에 있는 데이터를 NIC 버퍼에 직접 쓸 수 있으므로 단 한 번의 복사 작업으로 데이터를 네트워크에 전송할 수 있다.**

페이지 캐시와 zero copy의 조합을 통해 카프카 클러스터에서는 데이터가 페이지 캐시에서 직접 NIC 버퍼로 직접 전송되므로 디스크 읽기가 전혀 발생하지 않기도 한다.

Java의 zero copy는 [IBM 문서](https://developer.ibm.com/articles/j-zerocopy/) 를 참고한다.


# 등장 인물

- 브로커: Kafka가 설치/실행되는 노드
- 컨트롤러: Kafka 클러스터에서 장애처리 등을 담당하는 특수 브로커로 하나만 존재
- 주키퍼: Kafka 클러스터 구성 및 관리(구버전에서는 메시지의 offset 등 메타데이터도 관리했으나 0.9 이후로는 브로커에서 관리)

# 데이터 모델

## 토픽

컨수머 큐라고 할 수 있으며 컨수머가 토픽 별로 메시지를 읽어갈 수 있음

## 파티션

- 하나의 토픽을 여러 파티션으로 나눠서 병렬 송신/수신 가능
- 각 파티션은 브로커의 디렉터리로 매핑. 디렉터리마다 인덱스 파일, 데이터 파일 2개의 파일 핸들 점유
- 리플리케이션도 파티션 단위로 동작하며 하나는 파티션 리더, 나머지는 파티션 팔로워
- 파티션 별 리더 선출은 직렬로 수행되므로 리더 선출 소요 시간은 파티션 수에 비례하여 증가

## 오프셋

- 파티션 내 유일하며 순차적으로 증가하는 64비트 정수
- 컨수머는 오프셋으로 정해진 순서대로만 메시지를 가져갈 수 있음


# 고가용성

## Replication

- 리플리케이션은 파티션 단위로 수행. 즉 파티션 별로 리더, 팔로워 존재
- 리플리케이션 팩터는 토픽 별로 설정
- 클러스터에서는 무중단 설정 변경 가능
    - config/server.properties 에 있는 리플리케이션 팩터 변경 후 해당 브로커만 재시작 -> 클러스터 내 전체 브로커에서 반복
- ISR(In Sync Replication)
    - 리플리케이션 그룹. 따라서 파티션 별로 존재
    - ISR 내에 있는 팔로워만 해당 파티션의 리더 후보가 될 수 있음
    - 리더는 팔로워의 정상 동작여부를 체크하며, 일정 시간 정상 동작하지 않으면 ISR에서 제외

# z-node

- 유닉스 파일시스템의 i-node 처럼 주키퍼에서 정보를 관리하는 단위
- 주키퍼 shell(zkCli.sh)에서 `ls` 로 확인 가능

# 계속...
